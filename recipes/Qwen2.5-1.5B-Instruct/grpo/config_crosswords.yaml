model_name_or_path: Qwen/Qwen2.5-1.5B-Instruct
model_revision: main
torch_dtype: bfloat16
attn_implementation: sdpa
dataset_name: od2961/Guardian-cryptonite-official-split
dataset_prompt_column: problem
dataset_solution_column: answer
dataset_train_split: train
dataset_validation_split: validation
dataset_test_split: test
system_prompt: "You are an expert *cryptic-crossword solver*.\n\nDo this (repeat until\
  \ fully consistent):\n\nA) DEVICE TRIAGE\n  • List plausible devices from {anagram,\
  \ container, reversal, hidden, charade,\n    deletion, homophone, double def, &lit,\
  \ letter selection, substitution, abbr}.\n  • For each, quote the indicator word(s).\
  \ Reject with a reason.\n\nB) PARSE\n  • Mark the **definition** (start or end).\n\
  \  • Mark the **wordplay** with exact fodder + operations.\n\nC) CHECKS\n  • Enumeration\
  \ must match exactly.\n  • Letter accounting must be exact (anagram multiset or\
  \ stepwise build).\n\nD) DECIDE\n  • Pick the candidate best matching definition,\
  \ indicator(s), and enumeration.\n  • Do NOT assume anagram without a clear anagrind\
  \ and fully used fodder.\n\nE) RECONSIDER (if any check fails)\n  • Begin the next\
  \ <think> with: \"Wait, we need to reconsider. Let's think this through step by\
  \ step.\"\n  • Say why it failed, then re-run A–D with an alternative device/parse.\n\
  \nFORMAT (no deviations):\n  • Reasoning only in <think>…</think>\n  • Final entry\
  \ ONLY (UPPER-CASE) in <answer>…</answer>\n------------------------------------------------------------\n\
  HIDDEN\nClue: Close, as seen in plaNET EARly (4)\n<think>Device: HIDDEN; indicator\
  \ “as seen in”.\nDef: “Close”. Wordplay: hidden in “plaNET EARly” → NEAR.\nEnumeration:\
  \ (4) OK.</think>\n<answer> NEAR </answer>\n\nClue: Mix TEA for a hot drink (3)\n\
  <think>Device: ANAGRAM; indicator “Mix”. Fodder TEA → TEA.\nDef: “a hot drink”.\
  \ Accounting exact; (3) OK.</think>\n<answer> TEA </answer>\n\nClue: Pet, when back,\
  \ is a god (3)\n<think>Device: REVERSAL; indicator “when back”.\nWordplay: GOD ←\
  \ → DOG. Def: “Pet”. (3) OK.</think>\n<answer> DOG </answer>\n\nClue: Animal, by\
  \ the sound of “dear” (4)\n<think>Device triage: {homophone ✓ (“by the sound of”),\
  \ hidden ✗, anagram ✗, …}\nDef: “Animal”. Wordplay: “dear” (sounds like) → DEER.\
  \ Enumeration (4) OK.</think>\n<answer>DEER</answer>\n\nClue: Shoe liner at home\
  \ on fish (6)\n<think>Device triage: {hidden ? (“on” is not a hidden indicator),\
  \ anagram ✗ (no anagrind),\ncharade ✓ (“at home”=IN, “on”=next to), homophone ✗,\
  \ …}\nAttempt (HIDDEN) rejected: no indicator; also hidden spans don’t give (6).\n\
  Candidate attempt (wrong path): — fails enumeration/indicator, so we must rethink.\
  \ \nRe-evaluate as CHARADES: IN (“at home”) + SOLE (“fish”) → INSOLE.\nAccounting:\
  \ INSOLE letters: I N S O L E (6). Definition “Shoe liner” fits. Enumeration (6)\
  \ OK.</think>\n<answer>INSOLE</answer>"
bf16: true
use_vllm: true
do_eval: true
gradient_accumulation_steps: 256
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false
hub_model_id: od2961/Qwen2.5-1.5B-Open-R1-GRPO-Crosswords-v07
hub_strategy: every_save
learning_rate: 1.0e-05
adam_beta2: 0.99
adam_epsilon: 1.0e-06
log_completions: true
log_level: info
logging_first_step: true
logging_steps: 1
logging_strategy: steps
lr_scheduler_type: cosine
max_reason_tokens: 275
return_reason: true
max_prompt_length: 3000
max_completion_length: 320
max_steps: -1
num_generations: 8
num_train_epochs: 3
output_dir: data/Qwen2.5-1.5B-Open-R1-GRPO-Crosswords-v07
overwrite_output_dir: true
per_device_eval_batch_size: 8
per_device_train_batch_size: 4
push_to_hub: false
report_to:
  - wandb
reward_funcs:
  - pure_accuracy
reward_weights:
  - 1.0
kl_target: 0.07
init_kl_coeff: 3.0
kl_horizon: 50000
kl_ctl_step_size: 0.15
max_grad_norm: 0.15
vf_coef: 0.25
advantage_normalization: running
horizon: 1024
save_strategy: steps
save_steps: 50
evaluation_strategy: steps
eval_steps: 100000000
save_total_limit: 1000
seed: 42
warmup_ratio: 0.35
reward_normalization: true
gamma: 0.99
gae_lambda: 0.95
log_grad_norm_after_clip: true
log_kl_per_token: true
reward_clip:
  - 0
  - 1
advantage_clip:
  - -1
  - 1
clip_range: 0.05
clip_range_vf: 0.5

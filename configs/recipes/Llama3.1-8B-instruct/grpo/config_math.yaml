# Copyright 2025 Liv d'Aliberti
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

model_name_or_path: meta-llama/Llama-3.1-8B-Instruct
model_revision: main
overwrite_output_dir: true
resume_from_checkpoint: false
torch_dtype: float16
attn_implementation: sdpa
dataset_name: open-r1/OpenR1-Math-220k
zero3_save_16bit_model: true
dataset_prompt_column: problem
system_prompt: "You are an expert *mathematics problem-solver*.\n\nEvery time you\
  \ receive a problem you must:\n\u2022 Analyse it thoroughly.  \n  \u2013 Pinpoint\
  \ the **goal** (what quantity/set/form is requested).  \n  \u2013 Pinpoint the **givens/constraints**\
  \ (domains, integrality, non-negativity, geometric conditions).  \n  \u2013 Choose\
  \ the **methods** to apply (algebraic manipulation, factorization, inequalities,\
  \ counting, modular arithmetic, geometry, calculus, etc.).  \n  \u2013 Write out\
  \ the full derivation that leads to the final result.\n\n\u2022 Check that the result\
  \ satisfies all original constraints (no extraneous roots, correct domain, simplified\
  \ form, exact arithmetic).\n\n\u2022 Respond in **exactly** the tag-based format\
  \ shown below \u2013 no greeting, no commentary outside the tags.  \n  \u2013 The\
  \ final answer goes inside `<answer>` **only**.  \n  \u2013 Use **exact** math (fractions,\
  \ radicals, \u03C0, e). Avoid unnecessary decimals.  \n  \u2013 Canonical forms:\
  \ integers as plain numbers; reduced fractions a/b with b>0; simplified radicals;\
  \ rationalized denominators; sets/tuples with standard notation; intervals in standard\
  \ notation.  \n  \u2013 If there is **no solution**, write `NO SOLUTION`. If the\
  \ problem is **underdetermined**, write `I DON'T KNOW`.\n\n\u2022 You have a hard\
  \ cap of **600 output tokens**. Be concise but complete.\n\n------------------------------------------------------------\n\
  TAG TEMPLATE (copy this shape for every problem)\n<think>\nYOUR reasoning process\
  \ goes here:  \n1. quote the relevant bits of the problem  \n2. name the mathematical\
  \ tool(s) you apply  \n3. show each intermediate step until the result is reached\
  \  \n   \nIf you spot an error or an unmet constraint, iterate, repeating steps\
  \ 1\u20133 as many\ntimes as necessary until you are confident in your result. Finish\
  \ by verifying the\nresult satisfies the original conditions exactly (substitution/checks).\n\
  </think>\n<answer>\nTHEANSWER\n</answer>\n"
bf16: false
use_vllm: true
vllm_mode: colocate
do_eval: false
per_device_train_batch_size: 2
per_device_eval_batch_size: 2
gradient_accumulation_steps: 8
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false
train_grpo_objective: true
hub_model_id: od2961/Llama-8B-Open-R1-GRPO-math-v2
hub_strategy: every_save
push_to_hub: true
learning_rate: 5.0e-06
adam_beta2: 0.99
adam_epsilon: 1.0e-06
log_completions: true
log_level: info
logging_first_step: true
logging_steps: 1
logging_strategy: steps
lr_scheduler_type: cosine
max_prompt_length: 450
max_completion_length: 750
max_steps: -1
num_generations: 4
num_train_epochs: 3
output_dir: var/data/Llama-8B-Open-R1-GRPO-math-v2
report_to:
- wandb
reward_funcs:
- pure_accuracy_math
reward_weights:
- 1
kl_target: 0.07
beta: 0.50
kl_horizon: 50000
kl_ctl_step_size: 0.15
max_grad_norm: 0.15
vf_coef: 0.25
advantage_normalization: running
horizon: 1024
save_strategy: steps
save_steps: 50
eval_strategy: 'no'
eval_steps: 100
save_total_limit: 1000
seed: 42
warmup_ratio: 0.2
reward_normalization: true
gamma: 0.99
gae_lambda: 0.95
log_grad_norm_after_clip: true
log_kl_per_token: true
reward_clip:
- -6.0
- 6.0
advantage_clip:
- -2.5
- 2.5
ppo_clip_range: 0.10
clip_range_vf: 0.5

# Copyright 2025 Liv d'Aliberti
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Model arguments
model_name_or_path: Qwen/Qwen2.5-1.5B-Instruct
model_revision: main
resume_from_checkpoint: false
torch_dtype: float16
attn_implementation: sdpa

# Data training arguments
dataset_name: open-r1/OpenR1-Math-220k
dataset_prompt_column: problem
eval_dataset_name: HuggingFaceH4/MATH-500
eval_dataset_split: test
eval_dataset_prompt_column: problem
eval_dataset_solution_column: answer
disable_distributed_sampler: true
system_prompt: |
  You are an expert *mathematics problem-solver*.

  Every time you receive a problem you must:
  • Analyse it thoroughly.
    – Pinpoint the **goal** (what quantity/set/form is requested).
    – Pinpoint the **givens/constraints** (domains, integrality, non-negativity, geometric conditions).
    – Choose the **methods** to apply (algebraic manipulation, factorization, inequalities, counting, modular arithmetic, geometry, calculus, etc.).
    – Write out the full derivation that leads to the final result.

  • Check that the result satisfies all original constraints (no extraneous roots, correct domain, simplified form, exact arithmetic).

  • Respond in **exactly** the tag-based format shown below – no greeting, no commentary outside the tags.
    – The final answer goes inside `<answer>` **only**.
    – Use **exact** math (fractions, radicals, π, e). Avoid unnecessary decimals.
    – Canonical forms: integers as plain numbers; reduced fractions a/b with b>0; simplified radicals; rationalized denominators; sets/tuples with standard notation; intervals in standard notation.

  ------------------------------------------------------------
  TAG TEMPLATE (copy this shape for every problem)
  <think>
  YOUR reasoning process goes here:
  1. quote the relevant bits of the problem
  2. name the mathematical tool(s) you apply
  3. show each intermediate step until the result is reached

  If you spot an error or an unmet constraint, iterate, repeating steps 1–3 as many
  times as necessary until you are confident in your result. Finish by verifying the
  result satisfies the original conditions exactly (substitution/checks).
  </think>
  <answer>
  THEANSWER
  </answer>

# MaxEnt‑GRPO specific knobs (used by src/maxent_grpo/maxent_grpo.py)
# Enable MaxEnt weighting (set train_grpo_objective=false) with a modest tau and adaptive entropy target.
maxent_tau: 0.0001
maxent_q_temperature: 0.9       # soften/sharpen listwise q if rewards are flat/noisy
maxent_q_epsilon: 1.0e-6        # epsilon floor on q for full support
maxent_length_normalize_ref: true
maxent_logprob_chunk_size: 0  # finer chunking for ref scoring to trim peak memory
maxent_target_weight_entropy: 0.42  # ~70% of log(4) to keep weights spread but not uniform
maxent_tau_lr: 0.001
maxent_tau_min: 0.0001
maxent_tau_max: 1
maxent_tau_warmup_steps: 75    # roughly match LR warmup so tau stays steady early on
maxent_score_tail_tokens: 512  # restrict KL/entropy measurements to the tail of each completion
# Phase 2 meta-controller defaults (analytic updates for τ/β)
controller_meta_enabled: true
controller_meta_method: analytic
controller_meta_lr: 0.02
controller_meta_update_interval: 1
controller_meta_optimizer: sgd
controller_meta_truncation_steps: 1
maxent_use_clip_objective: true
maxent_clip_objective_coef: 1.0
maxent_clip_adv_baseline: 0.25   # 1/num_generations (4) for a gentler clip objective
train_grpo_objective: false

# InfoSeed (optional; defaults keep it off)
info_seed_enabled: false
info_seed_num_seeds: 0
info_seed_lambda: 0.0
info_seed_alpha_entropy: 0.0
info_seed_prompt_template: "\n[seed={seed}]"
info_seed_loss_type: infonce
info_seed_pooling: mean

# Training/eval & logging
bf16: true
fp16: false
use_vllm: true
vllm_mode: server         # reuse the single vLLM server from the launcher; avoids spawning 6 colocated engines
vllm_return_logprobs: true
vllm_gpu_memory_utilization: 0.9
do_eval: true
gradient_accumulation_steps: 32    # match TRL defaults for faster logging feedback
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false
gen_temperature: 0.7
gen_top_p: 0.8
gen_top_k: 32
gen_best_of: 1
vllm_stop_sequences:
  - "</answer>"
  - "</answer>\n"
  - "</assistant>"
vllm_request_timeout: 600
vllm_url: http://localhost:29525/generate
hub_model_id: od2961/Qwen2.5-1.5B-Open-R1-MaxEnt-GRPO-BASELINE-math-v1
hub_strategy: every_save
learning_rate: 5e-6
adam_beta2: 0.99
adam_epsilon: 1e-06
log_completions: true
log_level: info
logging_first_step: true
logging_steps: 1
logging_strategy: steps
log_like_grpo: true
lr_scheduler_type: cosine
max_prompt_length: 2500      # trim prompt window to ease ref scoring memory
max_completion_length: 2500   # shorter completions to reduce activation + ref costs
max_steps: 1000
# GRPO requires >=2 generations to compute advantages; keep this at 2+ even when reducing memory.
num_generations: 8          # reduce scoring footprint while satisfying GRPO minimum
num_train_epochs: 5
output_dir: var/data/Qwen2.5-1.5B-Open-R1-MaxEnt-GRPO-BASELINE-math-v1
overwrite_output_dir: true
per_device_eval_batch_size: 8
per_device_train_batch_size: 1
push_to_hub: true
report_to:
- wandb
wandb_project: huggingface
wandb_entity: ogd3-princeton-university
reward_funcs:
- pure_accuracy_math
reward_weights:
- 1

# Reverse‑KL weight β used inside MaxEnt update
init_kl_coeff: 0.50
init_kl_coef: 0.50
kl_penalty_beta: 0.50
kl_target:        0.07
kl_horizon:       5000
kl_ctl_step_size: 0.25

# Misc GRPO‑style knobs (not all consumed by the minimal script, kept for consistency)
max_grad_norm: 0.15
clip_range: 0.05
save_strategy: steps
save_steps: 50
evaluation_strategy: steps
eval_steps: 25
save_total_limit: 1000
seed: 42
warmup_ratio: 0.2

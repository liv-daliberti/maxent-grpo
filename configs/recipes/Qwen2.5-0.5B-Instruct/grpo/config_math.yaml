# Copyright 2025 Liv d'Aliberti
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Model arguments
model_name_or_path: Qwen/Qwen2.5-0.5B-Instruct
model_revision: main
resume_from_checkpoint: false
torch_dtype: float16
attn_implementation: sdpa

# Data training arguments
dataset_name: open-r1/OpenR1-Math-220k
dataset_prompt_column: problem
eval_dataset_name: HuggingFaceH4/MATH-500
eval_dataset_split: test
eval_dataset_prompt_column: problem
eval_dataset_solution_column: answer
disable_distributed_sampler: true
system_prompt: |
  You are an expert *mathematics problem-solver*.

  Every time you receive a problem you must:
  • Analyse it thoroughly.
    – Pinpoint the **goal** (what quantity/set/form is requested).
    – Pinpoint the **givens/constraints** (domains, integrality, non-negativity, geometric conditions).
    – Choose the **methods** to apply (algebraic manipulation, factorization, inequalities, counting, modular arithmetic, geometry, calculus, etc.).
    – Write out the full derivation that leads to the final result.

  • Check that the result satisfies all original constraints (no extraneous roots, correct domain, simplified form, exact arithmetic).

  • Respond in **exactly** the tag-based format shown below – no greeting, no commentary outside the tags.
    – The final answer goes inside `<answer>` **only**.
    – Use **exact** math (fractions, radicals, π, e). Avoid unnecessary decimals.
    – Canonical forms: integers as plain numbers; reduced fractions a/b with b>0; simplified radicals; rationalized denominators; sets/tuples with standard notation; intervals in standard notation.

  ------------------------------------------------------------
  TAG TEMPLATE (copy this shape for every problem)
  <think>
  YOUR reasoning process goes here:
  1. quote the relevant bits of the problem
  2. name the mathematical tool(s) you apply
  3. show each intermediate step until the result is reached

  If you spot an error or an unmet constraint, iterate, repeating steps 1–3 as many
  times as necessary until you are confident in your result. Finish by verifying the
  result satisfies the original conditions exactly (substitution/checks).
  </think>
  <answer>
  THEANSWER
  </answer>

# GRPO paired config (mirrors MaxEnt training settings)
# Training/eval & logging
bf16: true
fp16: false
use_vllm: true            # enable vLLM server path for generation
vllm_mode: server         # use external/inline server launched by the slurm helper
torch_compile: false      # disabled: torch.compile is unstable with DeepSpeed/ZeRO here
# Request vLLM logprob metadata so MaxEnt can reconstruct reference stats from vLLM.
# Request logprob metadata from vLLM (requires a server with logprob support).
vllm_return_logprobs: true
vllm_request_logprobs: true
# Force frozen reference-model scoring for KL/GRPO trust region.
maxent_reference_logprobs_source: model
vllm_sync_weights: false
vllm_gpu_memory_utilization: 0.9
do_eval: true
gradient_accumulation_steps: 32    # match TRL defaults for faster logging feedback
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false
train_grpo_objective: true
force_custom_loop: true
gen_temperature: 1.0
gen_top_p: 1.0
gen_top_k: 0
gen_best_of: 1
vllm_stop_sequences:
  - "</answer>"
  - "</answer>\n"
  - "</assistant>"
vllm_request_timeout: 600
vllm_url: http://localhost:29525/generate/
hub_model_id: od2961/Qwen2.5-0.5B-Open-R1-GRPO-BASELINE-math-v1
hub_strategy: every_save
learning_rate: 1e-6
optim: adamw_torch_fused
adam_beta2: 0.99
adam_epsilon: 1e-06
log_completions: true
log_level: info
logging_first_step: true
logging_steps: 1
logging_strategy: steps
log_like_grpo: true
lr_scheduler_type: cosine
max_prompt_length: 1024      # trim prompt window to ease ref scoring memory
max_completion_length: 512   # shorter completions to reduce activation + ref costs
max_steps: 200
# GRPO requires >=2 generations to compute advantages; keep this at 2+ even when reducing memory.
num_generations: 4           # reduce scoring footprint while satisfying GRPO minimum
num_train_epochs: 1
output_dir: var/data/Qwen2.5-0.5B-Open-R1-GRPO-BASELINE-math-v1
overwrite_output_dir: true
per_device_eval_batch_size: 8
per_device_train_batch_size: 2
push_to_hub: true
report_to:
- wandb
wandb_project: huggingface
wandb_entity: ogd3-princeton-university
reward_funcs:
- pure_accuracy_math
reward_weights:
- 1

# Reverse‑KL weight β used inside MaxEnt update
# Gentle KL controller: keep updates small even when KL is far from target.
beta: 0.5
kl_target:        0.07
kl_horizon:       500
kl_ctl_step_size: 1.0

# Misc GRPO‑style knobs (not all consumed by the minimal script, kept for consistency)
max_grad_norm: 0.15
ppo_clip_range: 0.05
save_strategy: steps
save_steps: 50
eval_strategy: steps
eval_steps: 25
save_total_limit: 1000
seed: 42
warmup_ratio: 0.2

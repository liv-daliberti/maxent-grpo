# Copyright 2025 Liv d'Aliberti
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Model arguments
model_name_or_path: Qwen/Qwen2.5-7B
model_revision: main
resume_from_checkpoint: false
torch_dtype: float16
attn_implementation: sdpa

# Data training arguments
dataset_name: open-r1/OpenR1-Math-220k
dataset_prompt_column: problem
eval_dataset_name: HuggingFaceH4/MATH-500
eval_dataset_split: test
eval_dataset_prompt_column: problem
eval_dataset_solution_column: answer
system_prompt: "You are an expert *mathematics problem-solver*.\n\n  Every time you receive a problem you must:\n  • Analyse it thoroughly.  \n    – Pinpoint the **goal** (what quantity/set/form is requested).  \n    – Pinpoint the **givens/constraints** (domains, integrality, non-negativity, geometric conditions).  \n    – Choose the **methods** to apply (algebraic manipulation, factorization, inequalities, counting, modular arithmetic, geometry, calculus, etc.).  \n    – Write out the full derivation that leads to the final result.\n\n  • Check that the result satisfies all original constraints (no extraneous roots, correct domain, simplified form, exact arithmetic).\n\n  • Respond in **exactly** the tag-based format shown below – no greeting, no commentary outside the tags.  \n    – The final answer goes inside `<answer>` **only**.  \n    – Use **exact** math (fractions, radicals, π, e). Avoid unnecessary decimals.  \n    – Canonical forms: integers as plain numbers; reduced fractions a/b with b>0; simplified radicals; rationalized denominators; sets/tuples with standard notation; intervals in standard notation.  \n    – If there is **no solution**, write `NO SOLUTION`. If the problem is **underdetermined**, write `I DON'T KNOW`.\n\n  • You have a hard cap of **750 output tokens**. Be concise but complete.\n\n  ------------------------------------------------------------\n  TAG TEMPLATE (copy this shape for every problem)\n  <think>\n  YOUR reasoning process goes here:  \n  1. quote the relevant bits of the problem  \n  2. name the mathematical tool(s) you apply  \n  3. show each intermediate step until the result is reached  \n     \n  If you spot an error or an unmet constraint, iterate, repeating steps 1–3 as many\n  times as necessary until you are confident in your result. Finish by verifying the\n  result satisfies the original conditions exactly (substitution/checks).\n  </think>\n  <answer>\n  THEANSWER\n  </answer>\n  "
# GRPO trainer config
bf16: false
fp16: true
use_vllm: true
vllm_mode: server         # reuse the single vLLM server launched by the script
vllm_gpu_memory_utilization: 0.9
do_eval: true
gradient_accumulation_steps: 64
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false
hub_model_id: od2961/Qwen2.5-7B-Open-R1-GRPO-math-v1
hub_strategy: every_save
learning_rate: 5e-6
adam_beta2: 0.99
adam_epsilon: 1e-06
log_completions: true
log_level: info
logging_first_step: true
logging_steps: 1
logging_strategy: steps
lr_scheduler_type: cosine
max_prompt_length: 1400
max_completion_length: 600
max_steps: -1
num_generations: 4
num_train_epochs: 3
output_dir: var/data/Qwen2.5-7B-Open-R1-GRPO-math-v1
overwrite_output_dir: true
per_device_eval_batch_size: 8
per_device_train_batch_size: 8
push_to_hub: true
report_to:
- wandb
reward_funcs:
- pure_accuracy_math
reward_weights:
- 1
kl_target:        0.07       # was 0.05 → aim for ≈40 % of previous KL
init_kl_coeff:    0.50
init_kl_coef:     0.50        # TRL controller alias
kl_penalty_beta:  0.50        # Additional TRL alias
kl_horizon:       50000    # was 100 000 → controller adapts in ~½ the steps
kl_ctl_step_size: 0.15      # was 0.05 → gentler per‑update β adjustments
max_grad_norm: 0.15
vf_coef: 0.25
advantage_normalization: running
horizon: 1024
save_strategy: steps
save_steps: 50
evaluation_strategy: steps
eval_steps: 25
save_total_limit: 1000
seed: 42
warmup_ratio: 0.2
reward_normalization: true
gamma: 0.99
gae_lambda: 0.95
log_grad_norm_after_clip: true
log_kl_per_token: true
reward_clip: [-6.0, 6.0]
advantage_clip: [-2.5, 2.5]
clip_range:     0.05          # keep or lower to 0.08
clip_range_vf:  0.5           # tighten critic clipping

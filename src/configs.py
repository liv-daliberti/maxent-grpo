# coding=utf-8
# Copyright 2025 Liv d'Aliberti
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Configuration dataclasses used across training and data loading.

These types extend TRL's config classes and add higher-level knobs used in this
repository (e.g., dataset mixtures, benchmark lists, chat template).
"""

from dataclasses import dataclass, field
from typing import Any, Optional

import trl


@dataclass
class DatasetConfig:
    """Configuration for a dataset inside a mixture.

    :ivar id: Dataset repository ID on the Hub (e.g., "org/name").
    :vartype id: str
    :ivar config: Optional dataset configuration name.
    :vartype config: str | None
    :ivar split: Split to load, defaults to "train".
    :vartype split: str
    :ivar columns: Optional list of column names to keep; if provided all
        datasets in the mixture must share the same set of columns.
    :vartype columns: list[str] | None
    :ivar weight: Optional sampling weight in (0, 1]; when specified, a
        subsample of that proportion is taken from the split.
    :vartype weight: float | None
    """

    id: str
    config: Optional[str] = None
    split: str = "train"
    columns: Optional[list[str]] = None
    weight: Optional[float] = None


@dataclass
class DatasetMixtureConfig:
    """Configuration for a mixture of datasets.

    :ivar datasets: List of constituent dataset configurations.
    :vartype datasets: list[DatasetConfig]
    :ivar seed: RNG seed used for shuffling and subsampling.
    :vartype seed: int
    :ivar test_split_size: Optional fraction in (0, 1) to create a train/test
        split after mixing; when ``None`` only a train split is returned.
    :vartype test_split_size: float | None
    """

    datasets: list[DatasetConfig]
    seed: int = 0
    test_split_size: Optional[float] = None


@dataclass
class ScriptArguments(trl.ScriptArguments):
    """Extended TRL ScriptArguments with dataset mixture support.

    :param dataset_name: Optional dataset name when not using mixtures.
    :type dataset_name: str | None
    :param dataset_mixture: Optional configuration for creating a dataset
        mixture with optional subsampling and column selection. Example schema::

          dataset_mixture:
            datasets:
              - id: dataset_id1
                config: config_name
                columns: [col1, col2]
                weight: 0.5
              - id: dataset_id2
                config: config_name
                columns: [col1, col2]
                weight: 0.5
            seed: 42
            test_split_size: 0.1

    :type dataset_mixture: dict[str, Any] | None
    :raises ValueError: If neither ``dataset_name`` nor ``dataset_mixture`` are
        provided, or if the mixture has inconsistent column sets.
    """

    # Override the dataset_name to make it optional
    dataset_name: Optional[str] = field(
        default=None, metadata={"help": "Dataset name. Can be omitted if using dataset_mixture."}
    )
    dataset_mixture: Optional[dict[str, Any]] = field(
        default=None,
        metadata={
            "help": (
                "Configuration for creating dataset mixtures with advanced options "
                "like shuffling."
            )
        },
    )

    def __post_init__(self):
        """Validate and normalize ``dataset_mixture`` into dataclasses.

        :raises ValueError: If the mixture payload is malformed or columns are
            inconsistent across datasets.
        """
        if self.dataset_name is None and self.dataset_mixture is None:
            raise ValueError("Either `dataset_name` or `dataset_mixture` must be provided")

        if self.dataset_mixture is not None:
            if not isinstance(self.dataset_mixture, dict) or "datasets" not in self.dataset_mixture:
                raise ValueError(
                    "dataset_mixture must be a dictionary with a 'datasets' key. "
                    "Expected format: {'datasets': [...], 'seed': int}"
                )

            datasets_list = []
            datasets_data = self.dataset_mixture.get("datasets", [])

            if isinstance(datasets_data, list):
                for dataset_config in datasets_data:
                    datasets_list.append(
                        DatasetConfig(
                            id=dataset_config.get("id"),
                            config=dataset_config.get("config"),
                            split=dataset_config.get("split", "train"),
                            columns=dataset_config.get("columns"),
                            weight=dataset_config.get("weight", 1.0),
                        )
                    )
            else:
                raise ValueError("'datasets' must be a list of dataset configurations")

            self.dataset_mixture = DatasetMixtureConfig(
                datasets=datasets_list,
                seed=self.dataset_mixture.get("seed", 0),
                test_split_size=self.dataset_mixture.get("test_split_size", None),
            )

            # Check that column names are consistent across all dataset configs
            columns_sets = [
                set(dataset.columns)
                for dataset in datasets_list
                if dataset.columns is not None
            ]
            if columns_sets:
                first_columns = columns_sets[0]
                if not all(columns == first_columns for columns in columns_sets):
                    raise ValueError(
                        (
                            "Column names must be consistent across all dataset "
                            "configurations in a mixture. "
                        )
                        + f"Found different column sets: {[list(cols) for cols in columns_sets]}"
                    )


# NOTE: Shared options could be added with a mixin to reduce code duplication
@dataclass
class GRPOConfig(trl.GRPOConfig):
    """Additional knobs for GRPO runs (callbacks, benchmarks, etc).

    :ivar benchmarks: Benchmark names (LightEval/extended) to run post‑training.
    :vartype benchmarks: list[str]
    :ivar callbacks: Callback identifiers to enable during training.
    :vartype callbacks: list[str]
    :ivar chat_template: Optional tokenizer chat template override.
    :vartype chat_template: str | None
    :ivar hub_model_revision: Target branch/revision to push to on the Hub.
    :vartype hub_model_revision: str | None
    :ivar num_completions_to_print: Number of completions to print for debug.
    :vartype num_completions_to_print: int
    :ivar overwrite_hub_revision: Whether to force‑overwrite an existing branch.
    :vartype overwrite_hub_revision: bool
    :ivar push_to_hub_revision: Whether to push to a non‑default branch.
    :vartype push_to_hub_revision: bool
    :ivar system_prompt: Optional system prompt used in benchmarking.
    :vartype system_prompt: str | None
    :ivar wandb_log_unique_prompts: Log unique prompts as separate W&B runs.
    :vartype wandb_log_unique_prompts: bool
    :ivar wandb_entity: Optional W&B entity.
    :vartype wandb_entity: str | None
    :ivar wandb_project: Optional W&B project.
    :vartype wandb_project: str | None
    :ivar wandb_run_group: Optional W&B run group.
    :vartype wandb_run_group: str | None
    """

    benchmarks: list[str] = field(
        default_factory=lambda: [],
        metadata={"help": "The benchmarks to run after training."},
    )
    callbacks: list[str] = field(
        default_factory=lambda: [],
        metadata={"help": "The callbacks to run during training."},
    )
    chat_template: Optional[str] = field(
        default=None,
        metadata={"help": "The chat template to use."},
    )
    hub_model_revision: Optional[str] = field(
        default="main", metadata={"help": "The Hub model branch to push the model to."}
    )
    num_completions_to_print: int = field(
        default=0,
        metadata={"help": "Number of completions to print."},
    )
    overwrite_hub_revision: bool = field(
        default=False,
        metadata={"help": "Whether to overwrite the Hub revision."},
    )
    push_to_hub_revision: bool = field(
        default=False,
        metadata={"help": "Whether to push to a Hub revision/branch."},
    )
    system_prompt: Optional[str] = field(
        default=None,
        metadata={"help": "The optional system prompt to use."},
    )
    wandb_log_unique_prompts: bool = field(
        default=True,
        metadata={
            "help": (
                "Whether to log the unique prompts to wandb. This will create a "
                "new run for each unique prompt."
            )
        },
    )
    wandb_entity: Optional[str] = field(
        default=None,
        metadata={"help": ("The entity to store runs under.")},
    )
    wandb_project: Optional[str] = field(
        default=None,
        metadata={"help": ("The project to store runs under.")},
    )
    wandb_run_group: Optional[str] = field(
        default=None,
        metadata={"help": ("The group to store runs under.")},
    )

@dataclass
class SFTConfig(trl.SFTConfig):
    """Additional knobs for SFT runs (callbacks, benchmarks, etc)."

    benchmarks: list[str] = field(
        default_factory=lambda: [],
        metadata={"help": "The benchmarks to run after training."},
    )
    callbacks: list[str] = field(
        default_factory=lambda: [],
        metadata={"help": "The callbacks to run during training."},
    )
    chat_template: Optional[str] = field(
        default=None,
        metadata={"help": "The chat template to use."},
    )
    system_prompt: Optional[str] = field(
        default=None,
        metadata={"help": "The optional system prompt to use for benchmarking."},
    )
    hub_model_revision: Optional[str] = field(
        default="main",
        metadata={"help": "The Hub model branch to push the model to."},
    )
    overwrite_hub_revision: bool = field(
        default=False,
        metadata={"help": "Whether to overwrite the Hub revision."},
    )
    push_to_hub_revision: bool = field(
        default=False,
        metadata={"help": "Whether to push to a Hub revision/branch."},
    )
    wandb_entity: Optional[str] = field(
        default=None,
        metadata={"help": ("The entity to store runs under.")},
    )
    wandb_project: Optional[str] = field(
        default=None,
        metadata={"help": ("The project to store runs under.")},
    )
    wandb_run_group: Optional[str] = field(
        default=None,
        metadata={"help": ("The group to store runs under.")},
    )


@dataclass
class GRPOScriptArguments(ScriptArguments):
    """
    Script arguments for the GRPO training script.

    Args:
        reward_funcs (`list[str]`):
            List of reward functions. Allowed: 'pure_accuracy_math' only.
        cosine_min_value_wrong (`float`):
            Minimum reward for cosine scaling for wrong answers.
        cosine_max_value_wrong (`float`):
            Maximum reward for cosine scaling for wrong answers.
        cosine_min_value_correct (`float`):
            Minimum reward for cosine scaling for correct answers.
        cosine_max_value_correct (`float`):
            Maximum reward for cosine scaling for correct answers.
        cosine_max_len (`int`):
            Maximum length for cosine scaling.
        max_completion_len (`int`):
            Maximum number of tokens in completion.
        soft_punish_cache (`int`):
            Minimum number of tokens in completion.
    """

    reward_funcs: list[str] = field(
        default_factory=lambda: ["pure_accuracy_math"],
        metadata={
            "help": "List of reward functions. Allowed: 'pure_accuracy_math' only."
        },
    )
    cosine_min_value_wrong: float = field(
        default=0.0,
        metadata={"help": "Minimum reward for wrong answers"},
    )
    cosine_max_value_wrong: float = field(
        default=-0.5,
        metadata={"help": "Maximum reward for wrong answers"},
    )
    cosine_min_value_correct: float = field(
        default=0.5,
        metadata={"help": "Minimum reward for correct answers"},
    )
    cosine_max_value_correct: float = field(
        default=1.0,
        metadata={"help": "Maximum reward for correct answers"},
    )
    cosine_max_len: int = field(
        default=1000,
        metadata={"help": "Maximum length for scaling"},
    )
    repetition_n_grams: int = field(
        default=3,
        metadata={"help": "Number of n-grams for repetition penalty reward"},
    )
    repetition_max_penalty: float = field(
        default=-1.0,
        metadata={"help": "Maximum (negative) penalty for for repetition penalty reward"},
    )
    # Removed: code_language, code_eval_*
    # and parallel_code_exec_per_proc (code-based rewards removed)

    dataset_prompt_column: str = field(
        default="problem",
        metadata={"help": "Column to use as prompts for training."},
    )
    dataset_solution_column: str = field(
        default="answer",
        metadata={"help": "Column to use as the gold solution/answer for training."},
    )

    # Removed: e2b/morph router URLs and provider settings (code execution removed)

    max_completion_len: int = field(
        default=16384,
        metadata={"help": "Maximum number of characters in completion."},
    )
    soft_punish_cache: int = field(
        default=4096,
        metadata={"help": "Minimum number of characters in completion."},
    )

    span_kl_target:   float = field(default=0.05, metadata={"help": "per-token KL target"})
    span_kl_beta0:    float = field(default=0.12, metadata={"help": "initial KL coeff"})
    span_kl_horizon:  int   = field(default=10000, metadata={"help": "horizon for KL controller"})

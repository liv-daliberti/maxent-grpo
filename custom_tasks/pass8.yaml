model_parameters:
  model_name: od2961/Qwen2.5-1.5B-Open-R1-GRPO-math-2k
  revision: main
  dtype: float16
  # Reduce vLLM init (KV cache + cudagraph capture) overhead.
  max_model_length: 8192

  # For chat/instruct models, enable the tokenizer chat template.
  use_chat_template: false
  generation_parameters:
    temperature: 0.7
    top_p: 0.95
    max_new_tokens: 4096
